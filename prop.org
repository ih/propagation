propagation-based inference
* motivation
** what is the relationship between learning, abstract knowledge, and problem solving
Learning can be considered the acquisition of abstract knowledge.  Abstractions provide constraints that allow for efficient inference/planning/reasoning/problem solving.  The propagation model presents a succint way to implement constraint-based computation.

In the larger scheme of things, the motivation for this project is to gain a better understanding of how abstraction/structure is used in problem solving.

Usually constraints come from the semantics of primitive operations i.e. knowing how to reverse their evaluation.  Abstraction provides high level operations and efficient inference/reasoning will be based on understanding how to reverse these operations.

"The main outstanding challenge is finding a means to “reverse” evaluation locally."
-Vikash Mansinghka


** propagation-based probabilistic inference 
The more immediate goal of the project is to implement probabilistic inference in the propagation model.  The basic intuition is to model the propagation of uncertainty through a computation, but use semantic information about operations in the computation (the structure) to constrain uncertainty and make inference more efficient.  We'll examine how uncertainty is propagated when semantics (inverse behavior) about primitive operations is sometimes known.

* background
** propagation networks
*** propagation networks: a flexible and expressive substrate for computation
alexey radul's phd thesis
**** abstract
a general framework for building systems that perform computation via propagation of information
****** what does it mean for a cell to accumulate information about a value?
"The novel insight that should finally permit computing with general-purpose propagation is that a cell should not be seen as storing a value, but as accumulating information about a value"
****** what does he mean by "there are no arbitrary important decisions?
"I argue that the structure of the prototype follows from the overarching principle of computing by propagation and of storage by accumulating information—there are no important arbitrary decisions."  Perhaps this refers to probabilistic computing and random choices made during computation.
**** DONE time for a revolution
what is an example of mergeable, partial information? how does the notion of computational time constrain computation expressiveness of computation? how is the merging mechanism made generic?
****** DONE expression evaluation has been wonderful
in the expression paradigm expressions are computations whose value or side effect is determined by evaluation.  expressions are either atomic or a combinations of subexpressions each of which are evaluated then combined in a fixed way.  these combination expressions can be thought of as high-level actions where the particular way of combining subexpressions is the abstracted out part of a computation. 
******** DONE why is an expression a combination of instructions acting on the results of subinstructions?
An example to think of is the if expression, where if is a combination of three subexpressions (that eventually can be reduced to instructions), the combination of instructions is the implementation/semantics of if and the subinstructions being acted upon are the subexpressions to the if statement
****** DONE but we want more
there are computational problems where a computation can be defined in terms of sub-computations that can be related in many different ways
******** DONE what is an example of a constraint satisfaction problem
finding values in a system of linear equations, we have variables with constraints on the possible values (the equations that must hold involving them)
******** DONE what is an example of automatic reasoning over declared logical relations?
finding values for variables that satisfy a logical formula? e.g. determining familial relationships like whether x is a grandfather of y, based on definition of grandfather in terms of a father relation and information about the father relation, propagation occurs in assigning truth to different possible statements.  the assignment of truth to particular statements affects what can be furthur deduced so order can matter.
********** DONE what is a logical relation?
a set of tuples, e.g. the father relation is a set of pairs (a,b) where a is the father of b
****** DONE we want more freedom from time
the sequential nature of evaluation-based computation constrains models of processes to be sequential, even if the true process is not well-ordered or is really several processes running concurrently
****** DONE propagation promises liberty
by organizing computation in terms of networks, one can automatically order operations performed based on the needs of the solution rather than the description of the problem.  this thesis presents a general-purpose propagation system for building such systems.
**** DONE design principles
****** DONE propagators are asynchronous, autonomous, and stateless
the general model is to have computing units called propagators connected to memory units called cells.  cells are responsible for maintaining locking mechanisms as well as detecting the end of a computation or quiescence 
******** how is this useful to ip?
in ip we can perhaps think of the computation for finding common subtrees a propagator and the parsing or the generation from a grammar of a model onto data a series of propagators, the cells would be incoming data as well as stored abstractions already formed
****** DONE we simulate the network until quiescence
computation is considered complete when a steady state has been reached (it seems a steady state occurs when cells become fixed), perhaps a better definition is when everything gets into a fixed cycle
****** DONE cells accumulate information
rather than store complete values cells store values in a range of development (from not started to partially computed to the final value of a computation) so that many different propagators may contribute to the computation of a particular value, in the ip setting we may see the program being parsed from a grammar as a partial value as different rules are activated and fit to data
******** DONE what does it mean for a cell to store information about a value
it's data that may or may not be useful in constructing the final value of the computation, one can think of it as results of partial computation(?)
******** DONE why don't cells store values?
using the design of "cells store values" leaves a lot of ambiguity as to how this should be implemented, e.g. what happens when we want to store a value to a cell that already has a value?  we can make an arbitrary decision here, but it's hard to tell what the long-term consequences will be.
******** DONE because trying to store values causes trouble
********** DONE option a
the problem with dropping values is we implicitly define special conditions for propagation (e.g. when to drop) and this violates our design goal of having the system be "always on"
************ DONE what are mutually inverse propagators?
basically inverse operations, e.g. in figure 2-2 we have diagram of a propagator system where given and two values in the circles the system computes the third value, the constraints of the operations are specified by saying what their inverses are and all these "mutually inverse" operators are propagators
********** DONE option b
the problem with always overwriting is loops will run forever, also changes in value introduce a notion of time (what the value was before and after the change)
********** DONE option c
throwing an error is not a good solution because it just moves the problem somewhere else like to the schedulers.  cells store state so this is where state should be dealt with.
********** DONE out of options
if we are to store values in cells and avoid the above problems we need to make storage decisions based on the content of the value, but this leads a brittleness in the system as demonstrated by the equality example
******** DONE and accumulating information is better
basically the answer is for cells to store everything that ever goes into them.  they should be thought of as places where information about a value is collected rather than the value itself
******** 
**** DONE core implementation
****** DONE numbers are easy to propagate
	      CLOSED: [2011-02-25 Fri 21:24]
conversion of F to C (temperature) is given as a wiring diagram illustrating how cells and propagators look in a concrete example.  this can be thought of as the execution of code rather than source.  
******** DONE an interlude on simulation
a scheduler is needed to tell when each propagator that is ready to run should be executed.  the simplest way is to have a queue, but things should be designed that order in the queue does not matter.  there is also a requirement that propagators perform all they need to do in a single execution, this may be limiting in the case we propagators that are doing large searches and are continuously outputting better and better results (does this violate the restriction?)
******** DONE making this work
cells need to accumulate information and also notify neighbors when changes have been made.  this is implemented through three functions add-content, new-neighbor!, and content.
********** DONE cells have three functions to interface with them
get the content, add content, register a propagator as a neighbor so whenever contents change the propagator is notified (scheduled to be run)
********** DONE cells were implemented as closures in the message-accepter style
this style has the object's (the cell) data (content) and functions (add content, content, add-neighbor) defined within a make-cell function.  

make-cell returns a messaging function which takes in a message saying what function should be called (much like object.method) these functions can manipulate the persistent data defined in the closure e.g. neighbors 

********** how is throwing an error in the definition of add-content different than "option C" of throwing an error in the design principles section discussing why cells should not store values? 
******** propagators
********* creating a general propagator
The general propagator constructor takes a list of neighbors and connects the propagator to the neighbors via [[*cells%20have%20three%20functions%20to%20interface%20with%20them][new-neighbor!]], it also makes sure to alert the propagator after all the connections have been made (even though the connection to each neighbor will alert the propagator i.e. add it to the job queue of the scheduler
********** DONE in the definition of propagator why does new-neighbor! take two arguments when the definition of new-neighbor! only takes one
actually the definition of new-neighbor! does take two arguments, the second is the propagator so to-do is the propagating function

********* creating a propagator with a specific function
calls the constructor for a general propagator where the neighbors are cells containing input to the function, an output cell is also passed to the function->propagator-constructor whose content is updated whenever the the propagator is alerted (this adding content to the output cell is the function passed to the general propagator constructor as the "propagator")
********* modifying functions to handle nothing as an input
instead of passing regular scheme functions to "function->propagator-constructor"s we'll want to make sure they can handle nothing values (in case their input cells have nothin).  this is done by wrapping the function with a case for handling nothing inputs

it's also worth noting if one wants to do probabilistic inference then rather then returning nothing when one of the inputs is nothing we can return a distribution over possible values
********* basic constructors
basic constructors e.g. + can be made by wrapping them in the do-nothin wrappers and passing them to function->propagator-constructor
		 
****** DONE propagation can go in any direction
	      CLOSED: [2011-02-26 Sat 22:27]
multi-directional networks can be built by encapsulating an operation's different directions of computation into a single function that creates propagators and their inverses e.g.
(define (product x y total)
  (multiplier x y total)
  (divider total y x)
  (divider total x y))
The key observation here is that the cells x, y, and total are shared between the different propagators and this is what makes the propagation paradigm so different from evaluation.

extending a network can be as easy as connecting a propagator to an existing cell like in the temperature conversion example of including kelvin conversion
****** DONE we can propagate intervals too
	      CLOSED: [2011-03-03 Thu 08:04]
propagation of information is demonstrated via an example for estimating the height of a building with a barometer (a niels bohr anectdote)

aside from the main illustration of performing computation on intervals and refining estimates through multiple sources of information, this example demonstrates a very interesting problem solving/reasoning capability of the networks where once one has a set of relations between different quantities one can easily make conditional inference using these propagation networks and this is very similar to human problem solving/lateral thinking (i.e. finding non-obvious relationships and exploiting them)
******* DONE making this work
	       CLOSED: [2011-03-03 Thu 08:04]
Operations (propagators) on intervals need to be added and cells need to be able to merge information i.e. changed so if they already contain something they can accept another input.
****** DONE generic operations let us propagate anything
       CLOSED: [2011-03-05 Sat 13:30]
******* DONE first cells
	CLOSED: [2011-03-05 Sat 13:30]
******** the context of merge 
the common part of dealing with integers and dealing with intervals was that each had its own way of combining new information with the current content of the cell, radul proposes abstracting this out via a merge function which will dispatch different policies for integrating information dependent on the type of information encountered.
******** generic operators
generic operators are implementations of an operator that makes it easy to dynamically specify operator behavior for different types of data.  more accurately it allows one to add functionality (separate from the operators definition via defhandler) where the functionality is executed dependent on predicates on the arguments, e.g. predicates may check the type of the arguments.  merge is implemented as a generic operator and the default case (predicates of any? and nothing? on the arguments) handles numbers or "complete information" and another set of cases handles intervals and interactions between interval and number
******* DONE then propagators
	       CLOSED: [2011-03-05 Sat 13:30]
adder, multiplier, etc propagators are redefined as generic operators with defhandlers added to each operation for interval computation.  a nice thing about the design of the system is multi-directional propagators such as sum, product etc do not have to be changed at all.

future propagation systems will be based on defining how merge should behave, but they will all fit under this unified framework of cells and propagators.
**** dependencies
*** revised report on the propagator model
by Alexey Radul and Gerald Jay Sussman
**** getting started
***** installation
the propagator network code can be downloaded from Gerald's home page

start scheme from the propagator home directory and type (load "load")
I had to restart scheme and do (load "load") again for it to work properly

***** basic example
you can create cells using (define-cell [name]) e.g. (define-cell a)

you can put something in a cell with (add-content [cell-name] [content])

there are operations for creating propagators and returning a cell that corresponds to the output e.g. (e:+ cell1 cell2)

to check what is in a cell use (content [cell-name])

you have to use (run) to have the network actually perform a computation
**** making propagator networks
propagators can be thought of as procedures and cells can be thought of as memory locations.  the difference between the traditional model of memory and propagation cells is that propagations cells  accumulate partial information instead of holding a value 
[???give a simple example]
***** attaching basic propagators: d@
the d@ operation (short for diagram apply) attaches a propagator to other cells with the convention being the last cell is the output cell for the propagator and the first argument is a cell that has propagator constructor information
***** propagator expressions: e@
****** what does the e@ operator do?
it connects a propagator to input cells then creates and returns the output cell
****** what is the motivation of the e@ operator?
a common case in building computations/networks is to have an intermediate value generated by one procedure/propagator feed directly into another procedure/propagator, the e@ operator allows for the construction of a network by chaining propagators in a lisp-like expression so you don't have to explicitly create intermediate output cells
****** why are d@ expressions still needed if e@ exists?
the d@ expression is useful for connecting cells that have already been constructed e.g. in building multidirectional networks
******* why do we need d@ used instead of just e@ for building multidirectional networks?
we specify the cells once (possibly using the e@ operator) for a forward computation, then we need to specify how the computation goes backward by connecting cells using the d@ operator
****** what is a shortcut for doing d@ operations when the propagator is known at construction time?
we can use the propagator constructor p:[propagator name] as the connecting operator i.e. (p:[propagator name] cell1 cell2 ...) is the same as (d@ p:[propagator name] cell1 cell2 ...)
******* why can't we always use the (p:prop-name ...) shortcut?
because we might only have the propagator name/type at run time (double check this)
***** late binding of application
****** how can a propagator not be known at network construction time?
a cell might be defined for a propagator and connected in a network without the cell actually having been assigned an operation/propagator
******* how does one assign a propagator to a cell?
(p:id p:[operator] cell)

***** provided primitives: p:foo and e:foo
****** what is the naming convention for p:foo and e:foo?
p: and e: do something with the contents of the passed in cells and write the output to a cell.  this output cell is the last argument of p:foo and created and returned by e:foo
****** what do p and e stand for in p:foo, e:foo?
propagator and expression, p:foo and e:foo are cells with information about propagator constructors
****** what does the id propagator do?
it continuously copies the contents of input to output
******* what is the signature of the id propagator?
(p:id input output), (e:id input)
****** what does the == propagator do?
it feeds the input cells to the output cell
******* what is the signature of the == propagator?
(p:== input ... output), (e:== input ...) 
****** what does the switch propagator do?
It allows for conditional copying of input to output cell based on a control cell having the value true
******* what is the signature of the switch propagator?
(p:switch control input output), (e:switch control input)
******* why does partial information make the switch propagator interesting?
the thing that is written to the output may be depedent on some conditions on the control cell
******** what is an example of this?
???
****** what does the conditional propagator do?
it is like an if expression where the first cell goes to the output cell if the control cell is true otherwise the second cell goes to the output cell
******* what is the signature of the conditional propagator?
(p:condtional control cell1 cell2 output)
(e:conditional control cell1 cell2)
****** what does the conditional-router propagator do?
this is like an if but instead the input goes to an output cell1 if the control is true and goes to output cell2 if the control is false
******* what is the signature of the conditional-router propagator?
(:conditional-router control input output1 output2)
(:conditional-router control input output1)
***** cells are data too
****** what is the implication of cells A and B being inside cell C?
they have partial information about the cell C
****** what is desirable if cells occur in another cell?
the inner cells should be kept in sync with c:id
******* what is meant by by partial information?
???
****** how can cells be added to a cell such that they are kept in sync?
using the deposit or examine propagator
******* what is the syntax of the deposit propagator?
(p:deposit cell place-cell) (e:deposit cell) where place-cell is the outer cell and cell is the inner cell
******* what is the syntax of the examine propagator?
it's deposit with the arguments reversed
(p:deposit place-cell cell), (e:examine place-cell)
******* what is the difference between examine and deposit?
the difference is deposit returns the outer cell and examine returns the inner cell

******** how does (e:examine place-cell) work?
if place-cell has a cell then it is returned, otherwise a new cell is synthesized then put into place-cell then returned
***** compound data
****** how is compound data made in the propagator model?
using the compound data structures of scheme to put together cells
******* what does the cons propagator do?
it creates a pair out of two cells and sticks them in an output cell using a syncing propagator like deposit
******** what is the syntax of the cons propagator?
(p:cons car-cell cdr-cell output), (e:cons car-cell cdr-cell)
******* what does the pair? propagator do?
attaches a propagator that tests whether the input contains a pair and writes a boolean to the output
******** what is the syntax of the pair? propagator?
(p:pair? input outout) (e:pair? input)
******* what does the null? propagator do?
attaches a propagator to the input that checks if it contains the empty list
******** what is the syntax for the null? propagator?
(p:null? input outout) (e:pair? input)
******* what does the car propagator do?
it syncs the car of a pair within the input to the passed in output cell.
******** what happens if a pair does not exist in the input cell?
a pair is created via (p: cons output nothing input)
******** what happens if a pair already exists in the input cell and e:car is used?
the car of the pair will be used as the output cell instead of synthesizing a new one
******** what is the syntax for the car propagator
(p:car input output), (e:car input)
******* what does the cdr propagator do?
it syncs the output to the cdr cell of the pair inside input, acts just like car
******** what is the syntax for the cdr propagator
(p:cdr input output), (e:cdr input)
******* what does it mean that merging by p:id is bidirectional?
not only does the contents of the input cell get continuously copied to the output cell, but any changes to the output cell get written to the input cell
******* what is an example of merging two cons structures together?
???
***** propagator constraints: c:foo and ce:foo
****** what are constraints for?
deriving information about the arguments of a function based on its output
****** what does c:foo do?
it attaches the propagator foo (like p:foo), but also attaches inverse propagators of foo
******* what do inverse propagators of foo do?
they deduce the inputs from the outputs
****** how does ce:foo work?
same as c:, but in expression style (so synthesizes the output cell)
***** constants and literal values
****** what is the preferred way to add a constant in a program?
use the constant propagator
******* what is the syntax for the constant propagator?
(p:constant constant-value) constructs the propagator
((p:constant constant-value) cell) propagates the constant value to a cell
(run)
******* how does the expression style constant propagator used?
(define-cell cell-name (e:constant constant-value))
(run)
***** constant conversion
****** what happens to constants in propagator programs?
the system will convert them to constant propagators when possible using e:constant
***** making cells
****** what are the equivalent of scheme variables in the propagator model?
cells bound to scheme variables
****** what is an alternative to (define-cell x)
(define x (make-cell))
******* what is the difference betwee (define-cell x) and (define x (make-cell))?
define-cell does constant conversion and adds meta-data for debugging

****** how do you define variables with let style in the propagator model?
(let-cells ((foo (e:+ x y))
           (bar (e:constant 5)))
...)

(let-cells* ((var val) (var val)...) ...)

(let-cells-rec ((var val) (var val)...) ...)

****** what is the initial state for any cell?
the partial information structure nothing
****** what does (let-cells (x y (foo (some thing))) ...) mean?
x and y are cells not attached to propagators (i.e. only have the nothing information in them.  it's like (let-cells ((x) (y) (foo (some thing))) ...)
****** what is the difference between let-cell and let-cells?
let-cell only allows for the creation of a single variable, in general just use let-cells
***** conditional network construction
****** what is the idea of conditional network construction?
delay the construction of the network conditioned upon information appearing at the boundary
****** what does the when propagator do?
it evaluates some code for constructing part of the network dependent on a control cell being true
******* what is the syntax for the when propagator?
(p:when internal-cells condition body ...)
where internal-cells are free variables in body and body is code that does some sort of network construction
condition is an expression that creates a cell with a conditional value
******** what is an example of the when propagator?
???
****** what does the unless propagator do?
same as the when propagator but reversing the control cell
****** what does the if propagator do?
evaluates one branch of network construction code with given input cells if control cell is true otherwise evaluates the other branch
**** making new compound propagators
***** what is a compound propagator?
a composition of several primitive propagators treated as a single propagator, much like a function can be thought of as a sequence of more primitive actions that gets abstracted/used as a single action
***** how do you create compound/abstract propagators?
define-d:propagator and define-e:propagator
****** what is the syntax for define-d:propagator and define-e:propagator?
(define-d:propagator ([name of new compound propagator] boundary_cell1 boundary_cell2 ...)
diagram-style propagator network made out of primitives...)
define-e:propagator is similar, but exprected to return an additional output cell and the network is defined in expression style
****** what is define-propagator short for?
define-d:propagator 
****** what is the syntax for anonymous propagators?
lambda-d:propagator and lambda-e:propagator
****** how does one use a propagator created using define-propagator?
(define-propagator (propagator-name...)...)

can be used as (p:propagator-name ...)
****** how does one create a constraint propagator using define-propagator?
(define-propagator (c:propagator-name ...) ...)
***** lexical scope
****** how are free variables accessed in a nested propagator definition?
using the import function e.g.
(define-propagator (a var1...)
  (define-propagator (b ...)
     (import var1) <--- gives access to the outer scope
))
***** recursion
****** Is "evaluation" (network construction) of a propagator lazy or eager?
eager
****** how is recursion implemented since evaluation is eager?
using conditional construction propagators like if, when, and unless (just like in scheme)
****** how would you write a network to compute the factorial function using diagram style propagators?
(define-propagator (p:factorial n n!) 
  (p:if (n n!) (e:= 0 n) 
    (p:== 1 n!) 
    (p:== (e:* n (e:factorial (e:- n 1))) n!))) 
****** how would you write a network to compute the factorial function using expression style propagators?
(define-e:propagator (e:factorial n) 
  (e:if (n) (e:= 0 n) 
    1 
    (e:* n (e:factorial (e:- n 1))))) 
**** using partial information
***** what is special about propagator memory cells?
they contain partial information about the value in the cell i.e. anything that is known about a value in a computation
***** how is partial information represented in the propagator system?
as scheme objects with a particular type that determines how the information interacts with everything else
***** what are examples of partial information?
*nothing* which indicates nothing is known about a value, a number which says the value is the number, an interval which says the value is bounded in a certain way
***** how is partial information put into a cell?
via add-content, which is used implicitly in define-cell
***** how do different types of partial information interact in a network?
they generally are coerced to the less-specific type, but the more specific information may narrow what is known
****** what is an example of how partial information mixes?
(define-cell x (make-interval 3 5)) 
(define-cell y (e:+ x 2)) 
then running the network gives
(content y)  ==>  #(interval 5 7) 
***** what is the key idea of partial information?
it accumulates so that one approaches the true value as more and more information is added
****** what is an example of how more information can narrow the possible values for a cell?
if (content y) => #interval (5 7) and we add more information
(add-content y (make-interval 4 6)) 
then 
(content y)  ==>  #(interval 5 6) 
***** what happens if two pieces of information about a value contradict each other?
the system will stop and complain
***** what functions are used to define partial information types?
equivalent?, merge, and contradictory?
****** what does equivalent? do?
it tests whether two pieces of partial information represent the same thing
****** what does merge do?
combines two information structures
****** what does contradictory? do?
tests whether an information structure represents an impossible state
***** what else is needed to define partial information types?
how propagators treat the type especially the behavior in the control position of a switch and the operator position of an apply
** probabilistic inference
*** church: a language for generative models
noah goodman, vikash mansighka, dan roy, keith bonawitz, josh tenenbaum 2008
an example of the sampling approach to inference
*** report on the probabilistic scheme
alexey radul 2007
an example of the systematic search approach to inference
**** introduction
***** what are some example applications of probabilistic modeling?
spam filtering, automated driving, discovering patterns of gene expression
****** how are probabilistic models used in spam filtering?
???
****** how are probabilistic models used in automated driving?
???
****** how are probabilistic models used in discovering patterns of gene expression?
???
***** what are the contributions of probabilistic scheme?
probabilistic scheme provides an embedding of probabilistic computation into a general-purpose programming language and anytime approximation using upper and lower bounds
****** what is meant by probabilistic computation?
determining a distribution over values for an expression, more specifically potentially complicated conditional distributions
****** what is meant by anytime approximation?
???
***** how does probabilistic scheme relate to probability distributions?
expressions in probabilistic scheme are distributions over the possible values the expression can evaluate to
***** how should one conceptualize distributions in probabilistic scheme?
as a list (more accurately a stream) of possibilities/possible values for an expression
***** what are the components of probabilistic scheme?
a stochastic language, an explicit language for distributions, and a query language
****** what is the stochastic language component of probabilistic scheme for?
constructing complex distribution as scheme programs with primitives nondeterministic primitives
****** what is the explicit language for distributions in probabilistic scheme for?
???
****** what is the query language for?
it's used for getting specific information out of/about a distribution
******* what is an example of information that would be queried from a distribution?
???
**** background
***** what is the motivation for probabilistic scheme?
allow modeling for more structure in domains than current probabilistic model representations
****** what is lacking in the bayesian network formulation of probabilistic models?
the ability to capture complex structure of a domain 
******* why do bayesian networks have a hard time capturing complex structure?
it is a propositional system and so has similar limitations as propositional logic (compared to first order logic)
******** why is propositional logic more limited than first order logic?
???
******* what is an example of relational structure?
???
******** why is this difficult to model with a Bayesian network?
???
******* what is an example of first-order logical structure?
???
******** why is this difficult to model with a Bayesian network?
???
****** how does probabilistic scheme improve on the ability to capture structure?
***** how does probabilistic scheme differ from IBAL?
it is not an entire new language rather it is embedded within a general purpose programming language
****** what is IBAL?
IBAL is an OCAML based probabilistic programming language
***** how does probabilistic scheme differ from Ramsey and Pfeffer stochastic lambda calculus?
it is based on a more operational approach to semantics rather than a denotational semantics
****** what are denotational semantics?
meaning of one expression is defined in terms of the meaning of an expression in another language
expressions are converted into terms in a mathematical language, this corresponds to the idea of compilation
******* what is an example of the denotational semantics of an expression?
???
****** what are operational semantics?
meaning of an expression is defined in terms of its execution on some machine

this corresponds to interpretation 
******* what is an example of the operational semantics of an expression?
???
**** stochastic language
***** what are the primitives for uncertainty in probabilistic scheme?
discrete-select, observe!, stochastic-thunk->distribution
****** what does discrete-select do?
returns an item with the probability specified
******* what is the signature of discrete-select?
(discrete-select (item1 prob1) (item2 prob2)...)
****** what does observe! do?
it forces the outcome of a stochastic expression to obey a given boolean expression and thus conditions the implicit distribution of the encapsulating expression
******* what is the signature of observe!
(observe! boolean)
****** what does stochastic-thunk->distribution do?
it returns an explicit distribution of a stochastic expression
******* what is the signature of stochastic-thunk->distribution?
(stochastic-thunk->distribution thunk)
******* how is an explicit distribution represented?
???
***** how is the conditional distribution p(dice-face|face>2) represented in probabilistic scheme?
the outcome,face, of a die roll is modeled then conditioned on face>2.  face is returned by the expression
****** how can a die be modeled in probabilistic scheme?
(define (roll-die) (discrete-select (1 1/6) (2 1/6) ... (6 1/6)))
****** how can the roll-die be conditioned on face being greater than 2
(let ((face (roll-die)))
(observe! (> face 2))
 face)
****** what is the implicit distribution for the expression (face (roll-die))?
uniform over die faces
****** what is the implicit distribution for the expression (> face 2)?
uniform over values of the die face greater than 2
****** how does (observe! (> face 2)) affect the implicit distribution?
it creates a conditional distribution 
***** how can the stochastic language be implemented with rejection sampling?
the stochastic primitives are implemented so that the thunk is a sampler and the frequency of the values it returns is an approximation of the distribution
****** how is discrete-select implemented in the rejection query paradigm?
discrete-select would randomly return a value
****** how is observe! implemented in the rejection query paradigm?
observe would throw an exception if it evaluated to false
****** how is stochastic-thunk->distribution implemented in the rejection query paradigm?
the thunk would be run several times and any successful values would be recorded

the frequencies of each returned value would be an approximation of the distribution
***** what is the general idea for the actual implementation of the stochastic language?
every possible outcome of the computation is explored as a possibility, these possibilities are either acceptable (according to the conditions) or not and give information on the overall distribution
****** how does finding the acceptable possibilities through systematic search give a distribution?
each possibility has an associated probability and once all the probabilities are known a distribution can be formed by normalizing them over the acceptable possibilities

* propagation-based probabilistic inference
The goal should be making random choices such that the condition holds.  the probability is then the product of the random choices.  In current sampling schemes this can be inefficient because the random choices are generally made independently (rejection sampling) or semi-randomly (gradient-style manipulation of a single random choice at a time, mcmc).  The potential of propagation is to use dependency information about the operations in the computation to make "smarter" random choices in order to force the condition to hold.

The above outlines how propagation can improve sampling approaches to inference, there also seems to be a way propagation can improve systematic search approaches.  ???How 
** arithmetic example
Let's look at the basic example presented in http://projects.csail.mit.edu/church/wiki/Conditioning
*** deterministic
(define (take-sample)
  (query
   (define A (if (flip) 1 0))
   (define B (if (flip) 1 0))
   (define C (if (flip) 1 0))
   (define D (+ A B C))
   A
   (equal? D 3)))

Here we can use the information about + and the condition being D==3 to force A,B, and C to take the value 1
**** possible methods of inference
***** "discard" approach
Goodman, Milch etc.
***** "systematic search"
We can propagate information forward about A,B, and C needing to take on value 1 or 0 and we can propagate information backward that D must be 3.  The semantics of + allows us to infer the values of A,B, and C must be 1.


*** uncertain
(define (take-sample)
  (query
   (define A (if (flip) 1 0))
   (define B (if (flip) 1 0))
   (define C (if (flip) 1 0))
   (define D (+ A B C))
   A
   (>= D 2)))

here once we make a random choice for A, B, or C we can force the choices for the other two values so that the condition holds using our knowledge of +

eventually we'd like to be able to learn constraints for higher level operations (possibly soft constraints or adding uncertainty to the semantics that propagates as well)
** probablistic programming tantalizes
section 5.2 of Alexey Radul's PhD thesis
*** how does dependency-directed backtracking improve probabilistic inference?
**** TODO how does dependency-directed backtracking work?
read in revised report on the propagator model
**** what is an example of evidence pushing in a propagation network?
**** what is an example of the "two levels of propagation" mentioned on page 85?

** propagation of uncertainty
*** nothing cases
can this be done by specifying how to handle [[*modifying%20functions%20to%20handle%20nothing%20as%20an%20input][nothing]] cases for functions?
*** [[*we%20can%20propagate%20intervals%20too][partial information]] and propagation of uncertainty
how is propagation of uncertainty formulated in the framework where distributions are partial programs, how does this relate to partial information

perhaps given data, different parts of the data are being explained/explained at the same time and all of this must be combined

