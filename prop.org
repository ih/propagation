propagation-based inference
* motivation
** what is the relationship between learning, abstract knowledge, and problem solving
Learning can be considered the acquisition of abstract knowledge.  Abstractions provide constraints that allow for efficient inference/planning/reasoning/problem solving.  The propagation model presents a succint way to implement constraint-based computation.

In the larger scheme of things, the motivation for this project is to gain a better understanding of how abstraction/structure is used in problem solving.

Usually constraints come from the semantics of primitive operations i.e. knowing how to reverse their evaluation.  Abstraction provides high level operations and efficient inference/reasoning will be based on understanding how to reverse these operations.

"The main outstanding challenge is finding a means to “reverse” evaluation locally."
-Vikash Mansinghka


** propagation-based probabilistic inference 
The more immediate goal of the project is to implement probabilistic inference in the propagation model.  The basic intuition is to model the propagation of uncertainty through a computation, but use semantic information about operations in the computation (the structure) to constrain uncertainty and make inference more efficient.  We'll examine how uncertainty is propagated when semantics (inverse behavior) about primitive operations is sometimes known.

* background
** propagation networks
*** propagation networks: a flexible and expressive substrate for computation
alexey radul's phd thesis
**** abstract
a general framework for building systems that perform computation via propagation of information
****** what does it mean for a cell to accumulate information about a value?
"The novel insight that should finally permit computing with general-purpose propagation is that a cell should not be seen as storing a value, but as accumulating information about a value"
****** what does he mean by "there are no arbitrary important decisions?
"I argue that the structure of the prototype follows from the overarching principle of computing by propagation and of storage by accumulating information—there are no important arbitrary decisions."  Perhaps this refers to probabilistic computing and random choices made during computation.
**** DONE time for a revolution
what is an example of mergeable, partial information? how does the notion of computational time constrain computation expressiveness of computation? how is the merging mechanism made generic?
****** DONE expression evaluation has been wonderful
in the expression paradigm expressions are computations whose value or side effect is determined by evaluation.  expressions are either atomic or a combinations of subexpressions each of which are evaluated then combined in a fixed way.  these combination expressions can be thought of as high-level actions where the particular way of combining subexpressions is the abstracted out part of a computation. 
******** DONE why is an expression a combination of instructions acting on the results of subinstructions?
An example to think of is the if expression, where if is a combination of three subexpressions (that eventually can be reduced to instructions), the combination of instructions is the implementation/semantics of if and the subinstructions being acted upon are the subexpressions to the if statement
****** DONE but we want more
there are computational problems where a computation can be defined in terms of sub-computations that can be related in many different ways
******** DONE what is an example of a constraint satisfaction problem
finding values in a system of linear equations, we have variables with constraints on the possible values (the equations that must hold involving them)
******** DONE what is an example of automatic reasoning over declared logical relations?
finding values for variables that satisfy a logical formula? e.g. determining familial relationships like whether x is a grandfather of y, based on definition of grandfather in terms of a father relation and information about the father relation, propagation occurs in assigning truth to different possible statements.  the assignment of truth to particular statements affects what can be furthur deduced so order can matter.
********** DONE what is a logical relation?
a set of tuples, e.g. the father relation is a set of pairs (a,b) where a is the father of b
****** DONE we want more freedom from time
the sequential nature of evaluation-based computation constrains models of processes to be sequential, even if the true process is not well-ordered or is really several processes running concurrently
****** DONE propagation promises liberty
by organizing computation in terms of networks, one can automatically order operations performed based on the needs of the solution rather than the description of the problem.  this thesis presents a general-purpose propagation system for building such systems.
**** DONE design principles
****** DONE propagators are asynchronous, autonomous, and stateless
the general model is to have computing units called propagators connected to memory units called cells.  cells are responsible for maintaining locking mechanisms as well as detecting the end of a computation or quiescence 
******** how is this useful to ip?
in ip we can perhaps think of the computation for finding common subtrees a propagator and the parsing or the generation from a grammar of a model onto data a series of propagators, the cells would be incoming data as well as stored abstractions already formed
****** DONE we simulate the network until quiescence
computation is considered complete when a steady state has been reached (it seems a steady state occurs when cells become fixed), perhaps a better definition is when everything gets into a fixed cycle
****** DONE cells accumulate information
rather than store complete values cells store values in a range of development (from not started to partially computed to the final value of a computation) so that many different propagators may contribute to the computation of a particular value, in the ip setting we may see the program being parsed from a grammar as a partial value as different rules are activated and fit to data
******** DONE what does it mean for a cell to store information about a value
it's data that may or may not be useful in constructing the final value of the computation, one can think of it as results of partial computation(?)
******** DONE why don't cells store values?
using the design of "cells store values" leaves a lot of ambiguity as to how this should be implemented, e.g. what happens when we want to store a value to a cell that already has a value?  we can make an arbitrary decision here, but it's hard to tell what the long-term consequences will be.
******** DONE because trying to store values causes trouble
********** DONE option a
the problem with dropping values is we implicitly define special conditions for propagation (e.g. when to drop) and this violates our design goal of having the system be "always on"
************ DONE what are mutually inverse propagators?
basically inverse operations, e.g. in figure 2-2 we have diagram of a propagator system where given and two values in the circles the system computes the third value, the constraints of the operations are specified by saying what their inverses are and all these "mutually inverse" operators are propagators
********** DONE option b
the problem with always overwriting is loops will run forever, also changes in value introduce a notion of time (what the value was before and after the change)
********** DONE option c
throwing an error is not a good solution because it just moves the problem somewhere else like to the schedulers.  cells store state so this is where state should be dealt with.
********** DONE out of options
if we are to store values in cells and avoid the above problems we need to make storage decisions based on the content of the value, but this leads a brittleness in the system as demonstrated by the equality example
******** DONE and accumulating information is better
basically the answer is for cells to store everything that ever goes into them.  they should be thought of as places where information about a value is collected rather than the value itself
******** 
**** DONE core implementation
****** DONE numbers are easy to propagate
	      CLOSED: [2011-02-25 Fri 21:24]
conversion of F to C (temperature) is given as a wiring diagram illustrating how cells and propagators look in a concrete example.  this can be thought of as the execution of code rather than source.  
******** DONE an interlude on simulation
a scheduler is needed to tell when each propagator that is ready to run should be executed.  the simplest way is to have a queue, but things should be designed that order in the queue does not matter.  there is also a requirement that propagators perform all they need to do in a single execution, this may be limiting in the case we propagators that are doing large searches and are continuously outputting better and better results (does this violate the restriction?)
******** DONE making this work
cells need to accumulate information and also notify neighbors when changes have been made.  this is implemented through three functions add-content, new-neighbor!, and content.
********** DONE cells have three functions to interface with them
get the content, add content, register a propagator as a neighbor so whenever contents change the propagator is notified (scheduled to be run)
********** DONE cells were implemented as closures in the message-accepter style
this style has the object's (the cell) data (content) and functions (add content, content, add-neighbor) defined within a make-cell function.  

make-cell returns a messaging function which takes in a message saying what function should be called (much like object.method) these functions can manipulate the persistent data defined in the closure e.g. neighbors 

********** how is throwing an error in the definition of add-content different than "option C" of throwing an error in the design principles section discussing why cells should not store values? 
******** propagators
********* creating a general propagator
The general propagator constructor takes a list of neighbors and connects the propagator to the neighbors via [[*cells%20have%20three%20functions%20to%20interface%20with%20them][new-neighbor!]], it also makes sure to alert the propagator after all the connections have been made (even though the connection to each neighbor will alert the propagator i.e. add it to the job queue of the scheduler
********** DONE in the definition of propagator why does new-neighbor! take two arguments when the definition of new-neighbor! only takes one
actually the definition of new-neighbor! does take two arguments, the second is the propagator so to-do is the propagating function

********* creating a propagator with a specific function
calls the constructor for a general propagator where the neighbors are cells containing input to the function, an output cell is also passed to the function->propagator-constructor whose content is updated whenever the the propagator is alerted (this adding content to the output cell is the function passed to the general propagator constructor as the "propagator")
********* modifying functions to handle nothing as an input
instead of passing regular scheme functions to "function->propagator-constructor"s we'll want to make sure they can handle nothing values (in case their input cells have nothin).  this is done by wrapping the function with a case for handling nothing inputs

it's also worth noting if one wants to do probabilistic inference then rather then returning nothing when one of the inputs is nothing we can return a distribution over possible values
********* basic constructors
basic constructors e.g. + can be made by wrapping them in the do-nothin wrappers and passing them to function->propagator-constructor
		 
****** DONE propagation can go in any direction
	      CLOSED: [2011-02-26 Sat 22:27]
multi-directional networks can be built by encapsulating an operation's different directions of computation into a single function that creates propagators and their inverses e.g.
(define (product x y total)
  (multiplier x y total)
  (divider total y x)
  (divider total x y))
The key observation here is that the cells x, y, and total are shared between the different propagators and this is what makes the propagation paradigm so different from evaluation.

extending a network can be as easy as connecting a propagator to an existing cell like in the temperature conversion example of including kelvin conversion
****** DONE we can propagate intervals too
	      CLOSED: [2011-03-03 Thu 08:04]
propagation of information is demonstrated via an example for estimating the height of a building with a barometer (a niels bohr anectdote)

aside from the main illustration of performing computation on intervals and refining estimates through multiple sources of information, this example demonstrates a very interesting problem solving/reasoning capability of the networks where once one has a set of relations between different quantities one can easily make conditional inference using these propagation networks and this is very similar to human problem solving/lateral thinking (i.e. finding non-obvious relationships and exploiting them)
******* DONE making this work
	       CLOSED: [2011-03-03 Thu 08:04]
Operations (propagators) on intervals need to be added and cells need to be able to merge information i.e. changed so if they already contain something they can accept another input.
****** DONE generic operations let us propagate anything
       CLOSED: [2011-03-05 Sat 13:30]
******* DONE first cells
	CLOSED: [2011-03-05 Sat 13:30]
******** the context of merge 
the common part of dealing with integers and dealing with intervals was that each had its own way of combining new information with the current content of the cell, radul proposes abstracting this out via a merge function which will dispatch different policies for integrating information dependent on the type of information encountered.
******** generic operators
generic operators are implementations of an operator that makes it easy to dynamically specify operator behavior for different types of data.  more accurately it allows one to add functionality (separate from the operators definition via defhandler) where the functionality is executed dependent on predicates on the arguments, e.g. predicates may check the type of the arguments.  merge is implemented as a generic operator and the default case (predicates of any? and nothing? on the arguments) handles numbers or "complete information" and another set of cases handles intervals and interactions between interval and number
******* DONE then propagators
	       CLOSED: [2011-03-05 Sat 13:30]
adder, multiplier, etc propagators are redefined as generic operators with defhandlers added to each operation for interval computation.  a nice thing about the design of the system is multi-directional propagators such as sum, product etc do not have to be changed at all.

future propagation systems will be based on defining how merge should behave, but they will all fit under this unified framework of cells and propagators.
**** dependencies
*** revised report on the propagator model
by Alexey Radul and Gerald Jay Sussman
**** getting started
***** installation
the propagator network code can be downloaded from Gerald's home page

start scheme from the propagator home directory and type (load "load")
I had to restart scheme and do (load "load") again for it to work properly

***** basic example
you can create cells using (define-cell [name]) e.g. (define-cell a)

you can put something in a cell with (add-content [cell-name] [content])

there are operations for creating propagators and returning a cell that corresponds to the output e.g. (e:+ cell1 cell2)

to check what is in a cell use (content [cell-name])

you have to use (run) to have the network actually perform a computation
**** making propagator networks
propagators can be thought of as procedures and cells can be thought of as memory locations.  the difference between the traditional model of memory and propagation cells is that propagations cells  accumulate partial information instead of holding a value 
[???give a simple example]
***** attaching basic propagators: d@
the d@ operation (short for diagram apply) attaches a propagator to other cells with the convention being the last cell is the output cell for the propagator and the first argument is a cell that has propagator constructor information
***** propagator expressions: e@
****** what does the e@ operator do?
it connects a propagator to input cells then creates and returns the output cell
****** what is the motivation of the e@ operator?
a common case in building computations/networks is to have an intermediate value generated by one procedure/propagator feed directly into another procedure/propagator, the e@ operator allows for the construction of a network by chaining propagators in a lisp-like expression so you don't have to explicitly create intermediate output cells
****** why are d@ expressions still needed if e@ exists?
the d@ expression is useful for connecting cells that have already been constructed e.g. in building multidirectional networks
******* why do we need d@ used instead of just e@ for building multidirectional networks?
we specify the cells once (possibly using the e@ operator) for a forward computation, then we need to specify how the computation goes backward by connecting cells using the d@ operator
****** what is a shortcut for doing d@ operations when the propagator is known at construction time?
we can use the propagator constructor p:[propagator name] as the connecting operator i.e. (p:[propagator name] cell1 cell2 ...) is the same as (d@ p:[propagator name] cell1 cell2 ...)
******* why can't we always use the (p:prop-name ...) shortcut?
because we might only have the propagator name/type at run time (double check this)
***** late binding of application
****** how can a propagator not be known at network construction time?
a cell might be defined for a propagator and connected in a network without the cell actually having been assigned an operation/propagator
******* how does one assign a propagator to a cell?
(p:id p:[operator] cell)

***** provided primitives: p:foo and e:foo
****** what is the naming convention for p:foo and e:foo?
p: and e: do something with the contents of the passed in cells and write the output to a cell.  this output cell is the last argument of p:foo and created and returned by e:foo
****** what do p and e stand for in p:foo, e:foo?
propagator and expression, p:foo and e:foo are cells with information about propagator constructors
****** what does the id propagator do?
it continuously copies the contents of input to output
******* what is the signature of the id propagator?
(p:id input output), (e:id input)
****** what does the == propagator do? 
******* what is the signature of the == propagator?
(p:== input ... output), (e:== input ...) 
****** what does the switch propagator do?
It allows for conditional copying of input to output cell based on a control cell having the value true
******* what is the signature of the switch propagator?
(p:switch control input output), (e:switch control input)
******* why does partial information make the switch propagator interesting?

****** what does the conditional propagator do?
******* what is the signature of the conditional propagator?
****** what does the conditional-router propagator do?
******* what is the signature of the conditional-router propagator?
** probabilistic inference
*** church: a language for generative models
noah goodman, vikash mansighka, dan roy, keith bonawitz, josh tenenbaum 2008
an example of the sampling approach to inference
*** TODO report on the probabilistic scheme
alexey radul 2007
an example of the systematic search approach to inference
* propagation-based probabilistic inference
The goal should be making random choices such that the condition holds.  the probability is then the product of the random choices.  In current sampling schemes this can be inefficient because the random choices are generally made independently (rejection sampling) or semi-randomly (gradient-style manipulation of a single random choice at a time, mcmc).  The potential of propagation is to use dependency information about the operations in the computation to make "smarter" random choices in order to force the condition to hold.

The above outlines how propagation can improve sampling approaches to inference, there also seems to be a way propagation can improve systematic search approaches.  ???How 
** arithmetic example
Let's look at the basic example presented in http://projects.csail.mit.edu/church/wiki/Conditioning
*** deterministic
(define (take-sample)
  (query
   (define A (if (flip) 1 0))
   (define B (if (flip) 1 0))
   (define C (if (flip) 1 0))
   (define D (+ A B C))
   A
   (equal? D 3)))

Here we can use the information about + and the condition being D==3 to force A,B, and C to take the value 1
**** possible methods of inference
***** "discard" approach
Goodman, Milch etc.
***** "systematic search"
We can propagate information forward about A,B, and C needing to take on value 1 or 0 and we can propagate information backward that D must be 3.  The semantics of + allows us to infer the values of A,B, and C must be 1.


*** uncertain
(define (take-sample)
  (query
   (define A (if (flip) 1 0))
   (define B (if (flip) 1 0))
   (define C (if (flip) 1 0))
   (define D (+ A B C))
   A
   (>= D 2)))

here once we make a random choice for A, B, or C we can force the choices for the other two values so that the condition holds using our knowledge of +

eventually we'd like to be able to learn constraints for higher level operations (possibly soft constraints or adding uncertainty to the semantics that propagates as well)
** probablistic programming tantalizes
section 5.2 of Alexey Radul's PhD thesis
*** how does dependency-directed backtracking improve probabilistic inference?
**** how does dependency-directed backtracking work?
**** what is an example of evidence pushing in a propagation network?
**** what is an example of the "two levels of propagation" mentioned on page 85?

** propagation of uncertainty
*** nothing cases
can this be done by specifying how to handle [[*modifying%20functions%20to%20handle%20nothing%20as%20an%20input][nothing]] cases for functions?
*** [[*we%20can%20propagate%20intervals%20too][partial information]] and propagation of uncertainty
how is propagation of uncertainty formulated in the framework where distributions are partial programs, how does this relate to partial information

perhaps given data, different parts of the data are being explained/explained at the same time and all of this must be combined

